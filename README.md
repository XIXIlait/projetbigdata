# ğŸ  Gardien d'une Maison ConnectÃ©e - Projet Big Data

## RÃ©sumÃ© du projet

Ce projet utilise **Kafka** et **Spark** pour surveiller en temps rÃ©el une maison connectÃ©e fictive.  
Des capteurs simulÃ©s envoient des Ã©vÃ©nements (tempÃ©rature, humiditÃ©, prÃ©sence, lumiÃ¨res) via Kafka,  
et Spark les analyse en continu pour dÃ©tecter des comportements et anomalies Ã©nergÃ©tiques.

## Architecture

```
Producteur Python â†’ Kafka Topic â†’ Spark Streaming â†’ Fichiers CSV
```

## Outils utilisÃ©s

- **Apache Kafka** : Message broker temps rÃ©el
- **Apache Spark** : Moteur d'analyse distribuÃ©
- **Docker / Docker Compose** : Orchestration des services
- **Python** : Producteur Kafka et job Spark

## Structure du projet

```
projetbigdata/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ sensor_producer.py
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_streaming_analysis.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ sample_events.csv
â”‚   â””â”€â”€ output/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_producer.sh
â”‚   â””â”€â”€ start_spark_job.sh
â””â”€â”€ screenshots/
```

## Installation

### 1. Clonage du dÃ©pÃ´t

```bash
git clone https://github.com/TON_USERNAME/projetbigdata.git
cd projetbigdata
```

### 2. CrÃ©ation et activation de l'environnement virtuel

```bash
python -m venv venv

# Windows (PowerShell)
venv\Scripts\activate

# Mac / Linux
source venv/bin/activate
```

### 3. Installation des dÃ©pendances

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

## Lancer le projet

### 1. DÃ©marrer lâ€™infrastructure Docker (Kafka + Zookeeper)

```bash
docker-compose up -d
docker-compose ps
```

Vous devez voir au minimum les services :

- `zookeeper` en **Up**
- `kafka` en **Up**

### 2. CrÃ©er le topic Kafka

```bash
docker exec -it kafka bash

# Dans le conteneur kafka :
kafka-topics --create --topic home_sensors --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

kafka-topics --list --bootstrap-server localhost:9092

exit
```

Vous devez voir `home_sensors` dans la liste.

### 3. Lancer le producteur (Terminal 1)

Assurez-vous que votre venv est activÃ©, puis :

```bash
cd projetbigdata   # si besoin
venv\Scripts\activate   # Windows

python producer/sensor_producer.py
```

Sortie attendue :

```text
ğŸš€ Producteur dÃ©marrÃ©. Envoi vers Kafka (localhost:9092)...
ğŸ“ Topic : home_sensors

[1] ğŸ“¤ Ã‰vÃ©nement envoyÃ© : living_room - temperature = 22.5
[2] ğŸ“¤ Ã‰vÃ©nement envoyÃ© : bedroom - humidity = 55.0
[3] ğŸ“¤ Ã‰vÃ©nement envoyÃ© : kitchen - presence = 1
...
```

### 4. Lancer Spark Streaming (Terminal 2)

Dans un **nouveau** terminal :

```bash
cd projetbigdata
venv\Scripts\activate   # Windows

python spark/spark_streaming_analysis.py
```

Sortie attendue :

```text
âœ… Spark Session crÃ©Ã©e
ğŸ“– Connexion Kafka Ã©tablie
ğŸ” SchÃ©ma parsÃ© :
root
 |-- room: string
 |-- sensor_type: string
 |-- value: double
 |-- timestamp: string
 |-- device_id: string

âœ… Spark Streaming dÃ©marrÃ©. Appuyez sur Ctrl+C pour arrÃªter.
```

Puis des tableaux de statistiques apparaissent rÃ©guliÃ¨rement dans la console.

### 5. RÃ©sultats gÃ©nÃ©rÃ©s

Les rÃ©sultats dâ€™analyse sont Ã©crits au format CSV dans :

```bash
data/output/anomalies/
```

Vous pouvez les afficher, par exemple :

```bash
dir data\output\anomalies   # Windows
cat data/output/anomalies/part-*.csv
```

### 6. ArrÃªt propre

```bash
# Terminal producteur
Ctrl+C

# Terminal Spark
Ctrl+C

# Services Docker
docker-compose down
```

## Captures dâ€™Ã©cran

Ã€ dÃ©poser dans le dossier `screenshots/` :

### Producteur en action

Le producteur envoie des Ã©vÃ©nements capteurs toutes les 2 secondes Ã  Kafka.

![Producer](./screenshots/producer.png)

### Spark Streaming

Spark reÃ§oit et analyse les donnÃ©es en temps rÃ©el et affiche les statistiques.

![Spark](./screenshots/spark.png)

### Docker containers

VÃ©rification que les services tournent correctement.

![Docker](./screenshots/docker.png)

### RÃ©sultats gÃ©nÃ©rÃ©s

Les fichiers CSV contenant les dÃ©tections / agrÃ©gations.

![Output](./screenshots/output.png)

## My Setup Notes

### DÃ©fi 1 : Communication Docker (rÃ©solu âœ…)

**ProblÃ¨me** : Au dÃ©part, Spark ne pouvait pas se connecter Ã  Kafka avec `localhost:9092` depuis certains environnements.  
**Cause** : En Docker, les conteneurs communiquent via les **noms de services** (`kafka`, `zookeeper`) et non via `localhost`.  
**Solution** : Utiliser `localhost:9092` cÃ´tÃ© scripts Python (car ils tournent en local) et sâ€™assurer que le `docker-compose.yml` expose bien ce port depuis le conteneur Kafka.  
**Apprentissage** : Toujours distinguer lâ€™hÃ´te (machine locale) et le rÃ©seau interne Docker.

### DÃ©fi 2 : Virtual Environment Python (rÃ©solu âœ…)

**ProblÃ¨me** : DifficultÃ©s Ã  installer / utiliser `pyspark`.  
**Cause** : Conflits de versions Python / Java ou absence de JDK dans certains cas.  
**Solution** : CrÃ©er un venv dÃ©diÃ© au projet, installer Java 11+ si nÃ©cessaire, puis installer `pyspark` dans ce venv.  
**Apprentissage** : Isoler les dÃ©pendances par projet simplifie Ã©normÃ©ment le dÃ©bogage.

## Concepts Big Data

1. **Streaming** : Traitement continu de flux de donnÃ©es (Ã©vÃ©nements capteurs en temps rÃ©el).  
2. **Windowing** : AgrÃ©gations sur des fenÃªtres temporelles (moyennes par minute, etc.).  
3. **ScalabilitÃ©** : Kafka + Spark peuvent gÃ©rer des volumes dâ€™Ã©vÃ©nements trÃ¨s importants.  
4. **Temps rÃ©el** : Faible latence entre la production de lâ€™Ã©vÃ©nement et son analyse.

## Auteur

Ton Nom â€“ DÃ©cembre 2025

## RÃ©fÃ©rences

- [Apache Kafka â€“ Documentation](https://kafka.apache.org/documentation/)
- [Apache Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
