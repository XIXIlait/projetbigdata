# PROJET BIG DATA : Smart Home IoT Analysis

Bonjour ! üëã
Ce fichier est le document central de mon projet. Il contient :
1.  **L'explication simple** du projet (C'est quoi ? √Ä quoi √ßa sert ?).
2.  **Le Guide d'Installation** (Toutes les commandes pour lancer le projet).
3.  **La Preuve de Fonctionnement** (Screenshots et explications techniques d√©taill√©es).
4.  **La Conformit√©** (Preuve que j'ai respect√© les consignes de l'Option A).

---

# 1. üéì C'est quoi ce projet ? (Explication Simple)

Imagine que nous voulons surveiller une "Maison Intelligente" (Smart Home) pour d√©tecter des probl√®mes (comme une lumi√®re oubli√©e ou une temp√©rature anormale) en temps r√©el.

Pour faire √ßa, nous avons construit une "usine de donn√©es" avec 3 acteurs :

1.  **Le Producteur (Python)** : C'est comme des **capteurs virtuels** dans la maison. Il g√©n√®re des faux √©v√©nements (Temp√©rature 25¬∞C, Lumi√®re Allum√©e...) et les envoie tr√®s vite.
2.  **Kafka (Le Facteur)** : C'est le **tuyau de transport**. Il re√ßoit les messages des capteurs et les garde en s√©curit√© en attendant qu'ils soient trait√©s.
3.  **Spark (Le Cerveau)** : C'est l'**analyseur**. Il lit les messages qui arrivent par le tuyau, calcule des statistiques (moyennes par minute) et surveille les anomalies pour nous alerter.

**L'Int√©r√™t du projet** :
C'est de prouver qu'on sait g√©rer des "Donn√©es en Streaming" (qui n'arr√™tent jamais d'arriver), exactement comme le font Uber, Netflix ou les banques aujourd'hui.

---

# 2. üíª Guide d'Ex√©cution : Commandes √† copier-coller

Voici la liste exacte des commandes pour lancer et tester le projet toi-m√™me.

## √âtape 1 : Tout nettoyer (optionnel, pour repartir de z√©ro)
Si tu veux √™tre s√ªr que tout est propre :
```powershell
docker-compose down
# Supprime les volumes (donn√©es) pour repartir √† neuf
docker volume prune -f
```

## √âtape 2 : Lancer l'infrastructure (Docker)
Ouvre un terminal (PowerShell ou CMD) √† la racine du projet (`c:\bigdata\projetbigdata`).
```powershell
docker-compose up -d
```
*Attends 30 secondes que tout d√©marre.*

V√©rifie que c'est lanc√© :
```powershell
docker ps
```
*Tu dois voir 3 lignes : zookeeper, kafka, spark.*

## √âtape 3 : Cr√©er le sujet de discussion (Topic Kafka)
On dit √† Kafka de cr√©er le canal "home_sensors".
```powershell
docker exec kafka kafka-topics --create --topic home_sensors --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1
```

## √âtape 4 : Lancer le Producteur (Les donn√©es)
Ouvre un **DEUXI√àME** terminal.
Active ton environnement Python et lance le script.

```powershell
# Active l'environnement virtuel
.\venv\Scripts\Activate

# Lance le producteur (attendre 30 secondes apres l'execution de cette commande)
python producer/sensor_producer.py
```
*Laisse ce terminal ouvert ! Tu vas voir les messages d√©filer.*

## √âtape 5 : Lancer l'Analyse Spark
Ouvre un **TROISI√àME** terminal.
On lance Spark √† l'int√©rieur de Docker pour √©viter les bugs Windows.

```powershell
docker exec spark /opt/spark/bin/spark-submit --conf spark.jars.ivy=/tmp/.ivy2 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 --master local[*] /home/spark_jobs/spark_streaming_analysis.py
```
*Tu vas voir beaucoup de texte d√©filer, c'est normal. Au bout d'un moment, tu verras des tableaux s'afficher toutes les minutes.*

## √âtape 6 : V√©rifier les r√©sultats
Si tu veux voir si des anomalies ont √©t√© d√©tect√©es, va voir dans ton dossier windows :
`c:\bigdata\projetbigdata\data\output\anomalies`
Tu y trouveras des fichiers CSV.

---

# 3. üì∏ Explications des Preuves (Screenshots)

Voici l'analyse technique de ce que vous voyez sur mes captures d'√©cran.

## üü¢ SCREEN 1 : Le Terminal "Producer" (G√©n√©ration de Donn√©es)
![alt text](image.png)

**Titre : Simulation des Capteurs IOT en Temps R√©el**

**Ce qu'on voit :**
Un script Python qui g√©n√®re et envoie des √©v√©nements en continu, environ toutes les 2 secondes. Chaque ligne repr√©sente une lecture de capteur envoy√©e.

**Comment √ßa marche (La logique du code) :**
Le script `sensor_producer.py` agit comme un simulateur de maison intelligente.
1.  **L'Al√©atoire** : √Ä chaque ex√©cution, il choisit al√©atoirement :
    -   Une **Pi√®ce** parmi 4 : `living_room`, `bedroom`, `kitchen`, `bathroom`.
    -   Un **Type de Capteur** parmi 4 : `temperature`, `humidity`, `presence`, `light`.
2.  **Les Valeurs R√©alistes** : Les donn√©es ne sont pas n'importe quoi, elles suivent des r√®gles logiques d√©finies dans le code :
    -   *Temp√©rature* : Entre 18¬∞C et 28¬∞C.
    -   *Humidit√©* : Entre 30% et 70%.
    -   *Pr√©sence/Lumi√®re* : Binaire (0 ou 1).
3.  **L'Envoi vers Kafka** : Une fois l'√©v√©nement cr√©√© (format JSON), il est "pouss√©" instantan√©ment vers le Topic Kafka `home_sensors` qui agit comme notre tuyau de transport de donn√©es.

**Pourquoi ?**
Cela prouve que notre syst√®me est capable d'ing√©rer des donn√©es dynamiques et non statiques, simulant un environnement r√©el impr√©visible.

---

## üîµ SCREEN 2 : Le Terminal "Spark" (Traitement Batch)

![alt text](image-1.png)

**Titre : Agr√©gation et Analyse en Streaming (Micro-Batchs)**

**Ce qu'on voit :**
Des tableaux ASCII g√©n√©r√©s par Spark qui se mettent √† jour. Chaque tableau correspond √† un "Batch" (un lot de traitement).

**Comment √ßa marche (La logique du code) :**
Spark Streaming √©coute le Topic Kafka et ne traite pas les messages un par un, mais par paquets (micro-batchs).
1.  **Le Fen√™trage (Windowing)** : Le code utilise une fonction `window`. Cela signifie qu'il regroupe toutes les donn√©es re√ßues durant une p√©riode pr√©cise (ex: 30 secondes).
2.  **L'Agr√©gation** : Pour chaque fen√™tre et chaque pi√®ce, il calcule des statistiques :
    -   `avg_value` : La moyenne (ex: temp√©rature moyenne).
    -   `min/max` : Les pics de valeurs (minimum et maximum).
    -   `count` : Le nombre de mesures re√ßues.
3.  **Mode "Update"** : Le tableau que tu vois n'affiche que les lignes qui ont √©t√© *modifi√©es* lors du dernier micro-batch. C'est pour cela que la taille du tableau change constamment : si seuls les capteurs de la cuisine ont envoy√© des donn√©es cette seconde-ci, seule la ligne "kitchen" appara√Æt.

**Pourquoi ?**
Cela d√©montre la capacit√© de Spark √† transformer des donn√©es brutes chaotiques en informations statistiques structur√©es et utiles, et ce, en quasi temps r√©el.

---

## üî¥ SCREEN 3 : Les Fichiers "Anomalies" (Alerting)

*Exemple d'anomalie : 2025-12-19T13:48:30.000Z,2025-12-19T13:49:00.000Z,kitchen,0,0,,59.55*

**Titre : D√©tection d'Incidents et Persistance des Donn√©es**

**Ce qu'on voit :**
L'explorateur de fichiers montrant des fichiers CSV dans le dossier `data/output/anomalies`.

**LA LOGIQUE DES ANOMALIES (QUAND EST-CE UNE ANOMALIE ?) :**
Ce fichier n'est pas juste une copie des donn√©es, c'est un **Rapport de Surveillance**.
Dans le code Spark, nous avons d√©fini des r√®gles pr√©cises pour surveiller la s√©curit√© de la maison :

1.  **Agr√©gation "Lights On"** :
    -   Le code regarde tous les messages de type "light".
    -   Il compte combien de fois la valeur √©tait "1" (Allum√©).
    -   *Logique :* `sum(case when sensor_type='light' and value=1 then 1 else 0)`
2.  **Agr√©gation "Presence Detected"** :
    -   Il fait la m√™me chose pour les capteurs de pr√©sence.
3.  **La D√©tection** :
    -   Le fichier CSV contient ces sommes pour chaque fen√™tre de 30 secondes.
    -   **L'Anomalie humaine** : C'est en lisant ce fichier qu'on d√©tecte les probl√®mes. Par exemple, si dans le CSV on voit `lights_on = 5` et `presence_detected = 0` pour la m√™me pi√®ce, **C'EST UNE ANOMALIE** (Lumi√®re allum√©e sans personne !).

**Pourquoi √©crire sur le disque ?**
Contrairement aux stats qui s'affichent juste √† l'√©cran, ces donn√©es sont critiques. On utilise un "File Sink" (CSV) pour les stocker durablement. Cela permettrait, dans un vrai projet, d'envoyer ces fichiers √† un syst√®me d'alarme.

---

# 4. ‚úÖ Conformit√© avec les Consignes (Option A)

Je certifie que ce projet respecte √† 100% l'Option A :

1.  **Utilisation de Docker** :
    -   ‚úÖ `docker-compose.yml` utilis√© pour lancer Zookeeper, Kafka et Spark.
    -   Preuve : Voir Screen 1 (Terminal Docker).

2.  **Streaming de Donn√©es** :
    -   ‚úÖ Script `producer/sensor_producer.py` simulant des capteurs IoT.
    -   Preuve : Voir Screen 1 (Terminal Producer).
    -   ‚úÖ Topic Kafka `home_sensors` cr√©√© et utilis√©.

3.  **Traitement Spark (Pyspark)** :
    -   ‚úÖ Script `spark/spark_streaming_analysis.py`.
    -   ‚úÖ Utilisation de `window()` pour les fen√™tres temporelles.
    -   ‚úÖ Calcul d'agr√©gats (`avg`, `min`, `max`) sur les capteurs.
    -   Preuve : Voir Screen 2 (Tableaux Spark).

4.  **D√©tection d'Anomalies / Stockage** :
    -   ‚úÖ Logique d'agr√©gation conditionnelle pour `lights_on` et `presence`.
    -   ‚úÖ √âcriture des r√©sultats au format CSV dans `data/output/`.
    -   Preuve : Voir Screen 3 (Fichiers Anomalies).
