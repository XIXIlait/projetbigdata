from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window, count, avg, max, min, sum as spark_sum
from pyspark.sql.functions import from_json, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
import time
import os

# CrÃ©er la session Spark
spark = SparkSession.builder \
    .appName("SmartHomeAnalysis") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "2") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("âœ… Spark Session crÃ©Ã©e")
print("ğŸ” Analyse des Ã©vÃ©nements capteurs de la maison connectÃ©e\n")

# DÃ©finir le schÃ©ma des Ã©vÃ©nements
schema = StructType([
    StructField("room", StringType(), True),
    StructField("sensor_type", StringType(), True),
    StructField("value", DoubleType(), True),
    StructField("timestamp", StringType(), True),
    StructField("device_id", StringType(), True)
])

INPUT_DIR = "data/streaming_input"
OUTPUT_DIR = "data/output"

# CrÃ©er les dossiers s'ils n'existent pas
os.makedirs(INPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(f"{OUTPUT_DIR}/stats", exist_ok=True)
os.makedirs(f"{OUTPUT_DIR}/anomalies", exist_ok=True)

print(f"ğŸ“ Lecture des donnÃ©es depuis : {INPUT_DIR}")
print(f"ğŸ“ Ã‰criture des rÃ©sultats dans : {OUTPUT_DIR}\n")

try:
    iteration = 0
    
    while True:
        iteration += 1
        print(f"\n{'='*60}")
        print(f"ğŸ”„ ITÃ‰RATION {iteration} - {time.strftime('%H:%M:%S')}")
        print(f"{'='*60}\n")
        
        # VÃ©rifier s'il y a des fichiers Ã  traiter
        files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.json')]
        
        if not files:
            print("â³ En attente de nouvelles donnÃ©es...")
            time.sleep(10)
            continue
        
        print(f"ğŸ“– {len(files)} fichier(s) de donnÃ©es trouvÃ©(s)")
        
        # Lire les donnÃ©es JSON
        df = spark.read \
            .schema(schema) \
            .json(f"{INPUT_DIR}/*.json")
        
        # Convertir le timestamp
        df = df.withColumn("timestamp_parsed", to_timestamp("timestamp"))
        
        total_events = df.count()
        print(f"ğŸ“Š Total d'Ã©vÃ©nements : {total_events}")
        
        if total_events == 0:
            print("âš ï¸  Aucun Ã©vÃ©nement Ã  traiter")
            time.sleep(10)
            continue
        
        # ANALYSE 1 : Statistiques par piÃ¨ce et type de capteur
        print("\nğŸ“ˆ STATISTIQUES PAR PIÃˆCE ET CAPTEUR:")
        stats_by_room = df \
            .filter(col("sensor_type").isin("temperature", "humidity")) \
            .groupBy("room", "sensor_type") \
            .agg(
                avg("value").alias("avg_value"),
                min("value").alias("min_value"),
                max("value").alias("max_value"),
                count("value").alias("count")
            ) \
            .orderBy("room", "sensor_type")
        
        stats_by_room.show(truncate=False)
        
        # Sauvegarder les statistiques
        stats_by_room.write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(f"{OUTPUT_DIR}/stats/stats_{int(time.time())}")
        
        print(f"ğŸ’¾ Statistiques sauvegardÃ©es dans {OUTPUT_DIR}/stats/")
        
        # ANALYSE 2 : DÃ©tection d'anomalies Ã©nergÃ©tiques
        print("\nğŸ” DÃ‰TECTION D'ANOMALIES Ã‰NERGÃ‰TIQUES:")
        
        # AgrÃ©gation par piÃ¨ce
        anomalies = df.groupBy("room").agg(
            spark_sum(
                (col("sensor_type") == "light").cast("int") * col("value")
            ).alias("lights_on_count"),
            
            spark_sum(
                (col("sensor_type") == "presence").cast("int") * col("value")
            ).alias("presence_detected_count"),
            
            avg(
                (col("sensor_type") == "temperature").cast("int") * col("value")
            ).alias("avg_temp"),
            
            avg(
                (col("sensor_type") == "humidity").cast("int") * col("value")
            ).alias("avg_humidity"),
            
            count("*").alias("total_events")
        ).orderBy("room")
        
        anomalies.show(truncate=False)
        
        # DÃ©tecter les anomalies (lumiÃ¨res allumÃ©es sans prÃ©sence)
        anomalies_detected = anomalies.filter(
            (col("lights_on_count") > 0) & (col("presence_detected_count") == 0)
        )
        
        anomaly_count = anomalies_detected.count()
        
        if anomaly_count > 0:
            print(f"âš ï¸  {anomaly_count} ANOMALIE(S) DÃ‰TECTÃ‰E(S) !")
            anomalies_detected.show(truncate=False)
        else:
            print("âœ… Aucune anomalie dÃ©tectÃ©e")
        
        # Sauvegarder les anomalies
        anomalies.write \
            .mode("append") \
            .option("header", "true") \
            .csv(f"{OUTPUT_DIR}/anomalies")
        
        print(f"ğŸ’¾ Anomalies sauvegardÃ©es dans {OUTPUT_DIR}/anomalies/")
        
        # ANALYSE 3 : Distribution des Ã©vÃ©nements
        print("\nğŸ“Š DISTRIBUTION DES Ã‰VÃ‰NEMENTS PAR TYPE:")
        df.groupBy("sensor_type").count().orderBy(col("count").desc()).show()
        
        print("\nğŸ“Š DISTRIBUTION DES Ã‰VÃ‰NEMENTS PAR PIÃˆCE:")
        df.groupBy("room").count().orderBy(col("count").desc()).show()
        
        # Archiver les fichiers traitÃ©s
        archive_dir = f"{INPUT_DIR}/processed"
        os.makedirs(archive_dir, exist_ok=True)
        
        for file in files:
            src = f"{INPUT_DIR}/{file}"
            dst = f"{archive_dir}/{file}"
            if os.path.exists(src):
                os.rename(src, dst)
        
        print(f"\nğŸ“¦ {len(files)} fichier(s) archivÃ©(s) dans {archive_dir}/")
        
        print(f"\nâ³ Prochaine analyse dans 15 secondes...")
        time.sleep(15)

except KeyboardInterrupt:
    print("\n\nâ¹ï¸  Analyse Spark arrÃªtÃ©e.")
    print("ğŸ“Š RÃ©sultats finaux disponibles dans :")
    print(f"   - {OUTPUT_DIR}/stats/")
    print(f"   - {OUTPUT_DIR}/anomalies/")

finally:
    spark.stop()
    print("\nâœ… Session Spark terminÃ©e")

